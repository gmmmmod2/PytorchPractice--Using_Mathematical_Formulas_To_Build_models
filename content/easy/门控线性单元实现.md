# 题目：门控线性单元(GLU)文本前馈块

> 目标 实现 GLU 前馈层，用于替代 ReLU 前馈：$ \mathrm{GLU}(A, B)= A \odot \sigma(B)$。

## 数学定义

设输入 $X\in\mathbb{R}^{B\times L\times d}$，中间维度 $m$：

$$
H = XW_1 + b_1,\quad G = \sigma(XW_2 + b_2),\quad
Y = (H \odot G )W_o + b_o
$$

## 实现要求

- 线性层分别输出 $m$ 维主分支与 $m$ 维 gate，再接输出投影到 $d$。
- `LayerNorm` 的 Pre-Norm / Post-Norm 可切换。
- 启用残差连接

## 参考实现

```python
import torch
import torch.nn as nn

class MyGLU(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, norm_mode: str):
        super().__init__()
        self.norm_mode = norm_mode
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.linear2 = nn.Linear(input_dim, hidden_dim)
        self.linear3 = nn.Linear(hidden_dim, input_dim)
        self.layerNorm = nn.LayerNorm(input_dim)

    def forward(self, x: torch.Tensor):
        """
        x: (B, L, d)
        """
        residual = x
        if self.norm_mode == 'pre':
            x = self.layerNorm(x)

        H = self.linear1(x)
        G = torch.sigmoid(self.linear2(x))
        Y = self.linear3(H * G)

        if self.norm_mode == 'post':
            Y = self.layerNorm(Y)
        return residual + Y
```
