# 题目（hard | NLP 可用 | 从公式到实现）

**手搓 1D 卷积（Conv1d）前向 + 反向传播**：实现一个不依赖 `nn.Conv1d` / `F.conv1d` 的 **自定义 autograd**。允许使用基础张量算子与 `unfold`/`as_strided`（任选其一），但不得调用任何现成卷积 API。

## 设定

输入 $X\in\mathbb{R}^{B\times C_{\text{in}}\times L_{\text{in}}}$，权重 $W\in\mathbb{R}^{C_{\text{out}}\times C_{\text{in}}\times K}$，偏置 $b\in\mathbb{R}^{C_{\text{out}}}$（可选）。

超参：填充 $p$、步幅 $s$、扩张 $d$（单值标量）。

有效输出长度：

$$
L_{\text{out}}=\left\lfloor\frac{L_{\text{in}}+2p-d\cdot (K-1)-1}{s}\right\rfloor+1
$$

前向（互相关形式）：

$$
Y_{b,c_o,t}=\sum_{c_i=1}^{C_{\text{in}}}\sum_{k=0}^{K-1}
W_{c_o,c_i,k}\;\cdot\;X^\text{pad}_{b,c_i,\;t\cdot s+k\cdot d}+b_{c_o}
$$

其中 $X^\text{pad}$ 为两端 $p$ 的零填充。

## 需推导并实现的梯度

令损失为 $\mathcal{L}$，写出并实现下列梯度（**请与代码一致**）：

先定义

$$
k^{\ast}\triangleq \frac{u+p-t\,s}{d}.
$$

则对输入的梯度为

$$
\frac{\partial \mathcal{L}}{\partial X_{b,c_i,u}}=
\sum_{c_o=1}^{C_{\mathrm{out}}}\sum_{t=0}^{L_{\mathrm{out}}-1}
\frac{\partial \mathcal{L}}{\partial Y_{b,c_o,t}}\cdot
\begin{cases}
W_{c_o,c_i,k^{\ast}}, & \text{若 } k^{\ast}\in\{0,\ldots,K-1\}\ \text{且 }(u+p-t\,s)\equiv 0 \pmod{d},\\[4pt]
0, & \text{否则}.
\end{cases}
$$

对权重：

$$
\frac{\partial \mathcal{L}}{\partial W_{c_o,c_i,k}}=
  \sum_{b=1}^B\sum_{t=0}^{L_{\text{out}}-1}
  \frac{\partial \mathcal{L}}{\partial Y_{b,c_o,t}}\;
  \cdot\;
  X^\text{pad}_{b,c_i,\;t\cdot s+k\cdot d}
$$

即用输出梯度在输入上的对应“切片”做逐元素乘加。

对偏置：$\displaystyle \frac{\partial \mathcal{L}}{\partial b_{c_o}}=\sum_{b,t}\frac{\partial \mathcal{L}}{\partial Y_{b,c_o,t}}$。

## 要求

1. 用 `torch.autograd.Function` 自定义 `forward`/`backward`，并封装成 `MyConv1d` 模块（带 `bias=True/False`）。
2. 支持超参：`padding=p`、`stride=s`、`dilation=d`。
3. 通过**数值梯度**或**与 `nn.Conv1d` 的梯度对比**进行校验（实现仍不可调用其前向）。
4. 代码需有**最小可复现自测**（包括随机张量、梯度检查、形状断言）。
5. 不考虑分组卷积（`groups=1`）。

---

# 答案（参考实现）

> 实现思路
>
> - **前向**：用 `unfold` 做 1D im2col（带扩张、步幅、填充），得到形状 `(B, C_in*K, L_out)` 的局部感受野，再做批量矩阵乘。
> - **反向 w.r.t. W/b**：根据上式，`grad_W = grad_Y @ col_X^T` 的适配重排；`grad_b = grad_Y` 在 `B,L_out` 两维求和。
> - **反向 w.r.t. X**：先将 `grad_Y` 右乘 `W`（按 im2col 展开后的排列）得到对 `col_X` 的梯度，再把它 **fold** 回输入形状。`fold` 可用 `as_strided` 或者手写加和；这里用“反向的 unfold”策略：构造全零张量，把梯度窗口按相同步幅/扩张/填充映射回去并累加（避免 `F.fold` 的 2D 限制）。

```python
import math
import torch
from torch import nn
from torch.autograd import Function

def _unfold1d(x, kernel_size, dilation=1, padding=0, stride=1):
    """
    x: (B, C, L_in) -> col: (B, C*kernel_size, L_out)
    等价于 nn.Unfold 的 1D 版本（自己实现，便于控制）
    """
    B, C, Lin = x.shape
    K = kernel_size
    # 计算输出长度
    Lout = (Lin + 2*padding - dilation*(K-1) - 1) // stride + 1
    # pad
    if padding > 0:
        x = torch.nn.functional.pad(x, (padding, padding))
        Lin = x.shape[-1]
    # 采样索引：每个感受野位置的起点
    idx = torch.arange(0, Lout*stride, stride, device=x.device)  # (L_out,)
    # 针对每个 kernel 位置的偏移（考虑扩张）
    koffs = torch.arange(0, dilation*K, dilation, device=x.device)  # (K,)

    # gather：我们需要 (B, C, K, L_out)
    # 先构造 (K, L_out) 的采样位置，再扩展到 (B,C,K,L_out)
    pos = idx.unsqueeze(0) + koffs.unsqueeze(1)  # (K, L_out)
    # clamp 安全（理论上不需要，因为 padding 后恰好覆盖）
    pos = pos.clamp(0, Lin-1)

    # 利用高级索引采样
    x_exp = x.unsqueeze(2)  # (B, C, 1, Lin)
    pos = pos.unsqueeze(0).unsqueeze(0).expand(B, C, -1, -1)  # (B,C,K,L_out)
    col = x_exp.gather(dim=3, index=pos)  # (B, C, K, L_out)

    # 重排到 (B, C*K, L_out)
    col = col.reshape(B, C*K, Lout)
    return col

def _fold1d(col, output_size, kernel_size, dilation=1, padding=0, stride=1, C=None):
    """
    col: (B, C*K, L_out) -> x_grad: (B, C, L_in)
    反向的 unfold：把每个位置的梯度窗口加回去（overlap-add）
    需要指定 output_size=(B,C,L_in) 或至少 L_in 与 C。
    """
    B, CK, Lout = col.shape
    K = kernel_size
    assert CK % K == 0, "CK must be divisible by kernel size"
    if C is None:
        C = CK // K
    Lin = output_size[-1]
    # 先得到 padding 后的长度
    Lin_pad = Lin + 2*padding
    # 准备输出（带 pad）
    x = col.new_zeros((B, C, Lin_pad))

    # 重塑回 (B, C, K, L_out)
    col = col.view(B, C, K, Lout)

    # 与 unfold 中一致的采样位置
    idx = torch.arange(0, Lout*stride, stride, device=col.device)  # (L_out,)
    koffs = torch.arange(0, dilation*K, dilation, device=col.device)  # (K,)
    pos = idx.unsqueeze(0) + koffs.unsqueeze(1)  # (K, L_out)
    pos = pos.clamp(0, Lin_pad-1)
    pos = pos.unsqueeze(0).unsqueeze(0).expand(B, C, -1, -1)  # (B,C,K,L_out)

    # 反向 scatter-add 到带 pad 的输出
    x.scatter_add_(dim=2, index=pos, src=col)  # (B,C,Lin_pad)

    # 去掉 padding
    if padding > 0:
        x = x[:, :, padding:-padding]
    return x  # (B,C,Lin)

class MyConv1dFn(Function):
    @staticmethod
    def forward(ctx, x, w, b=None, stride=1, padding=0, dilation=1):
        """
        x: (B, Cin, Lin)
        w: (Cout, Cin, K)
        b: (Cout,) or None
        """
        B, Cin, Lin = x.shape
        Cout, Cin_w, K = w.shape
        assert Cin == Cin_w, "Cin mismatch"

        # im2col
        col = _unfold1d(x, kernel_size=K, dilation=dilation, padding=padding, stride=stride)  # (B, Cin*K, Lout)
        Lout = col.shape[-1]

        # 重排权重以做 (B, Cout, Lout) = (Cout, Cin*K) @ (B, Cin*K, Lout)
        w_mat = w.view(Cout, Cin*K)  # (Cout, Cin*K)
        y = torch.einsum('oc,bcl->bol', w_mat, col)  # (B, Cout, Lout)

        if b is not None:
            y = y + b.view(1, -1, 1)

        # 保存反传所需
        ctx.save_for_backward(x, w, b, col)
        ctx.hyper = (stride, padding, dilation, Lout)
        return y

    @staticmethod
    def backward(ctx, grad_y):
        """
        grad_y: (B, Cout, Lout)
        返回顺序与 forward 输入一致：
        d_x, d_w, d_b, None, None, None
        """
        x, w, b, col = ctx.saved_tensors
        stride, padding, dilation, Lout = ctx.hyper
        B, Cin, Lin = x.shape
        Cout, _, K = w.shape

        # === grad w.r.t. bias ===
        grad_b = None
        if b is not None:
            # sum over batch and length
            grad_b = grad_y.sum(dim=(0, 2))  # (Cout,)

        # === grad w.r.t. weight ===
        # col: (B, Cin*K, Lout), grad_y: (B, Cout, Lout)
        # -> grad_w_mat: (Cout, Cin*K)
        grad_w_mat = torch.einsum('bol,bcl->oc', grad_y, col)  # (Cout, Cin*K)
        grad_w = grad_w_mat.view(Cout, Cin, K)

        # === grad w.r.t. x ===
        # 先得到对 col 的梯度： (B, Cin*K, Lout) = (B, Cout, Lout) 与 (Cout, Cin*K)
        w_mat = w.view(Cout, Cin*K)  # (Cout, Cin*K)
        grad_col = torch.einsum('bol,oc->bcl', grad_y, w_mat)  # (B, Cin*K, Lout)

        # fold 回输入形状
        grad_x = _fold1d(
            grad_col,
            output_size=(B, Cin, Lin),
            kernel_size=K,
            dilation=dilation,
            padding=padding,
            stride=stride,
            C=Cin
        )  # (B, Cin, Lin)

        return grad_x, grad_w, grad_b, None, None, None

class MyConv1d(nn.Module):
    def __init__(self, Cin, Cout, K, stride=1, padding=0, dilation=1, bias=True):
        super().__init__()
        self.Cin, self.Cout, self.K = Cin, Cout, K
        self.stride, self.padding, self.dilation = stride, padding, dilation
        # Kaiming 初始化
        w = torch.empty(Cout, Cin, K)
        nn.init.kaiming_uniform_(w, a=math.sqrt(5))
        self.weight = nn.Parameter(w)
        if bias:
            self.bias = nn.Parameter(torch.zeros(Cout))
        else:
            self.register_parameter('bias', None)

    def forward(self, x):
        return MyConv1dFn.apply(
            x, self.weight, self.bias, self.stride, self.padding, self.dilation
        )

# ------------------ 自测（梯度/数值对齐） ------------------
if __name__ == "__main__":
    torch.manual_seed(0)
    device = "cuda" if torch.cuda.is_available() else "cpu"

    B, Cin, Cout, Lin, K = 2, 3, 4, 17, 5
    stride, padding, dilation = 2, 1, 2

    x = torch.randn(B, Cin, Lin, device=device, dtype=torch.double, requires_grad=True)
    conv = MyConv1d(Cin, Cout, K, stride=stride, padding=padding, dilation=dilation, bias=True).to(device).double()

    # 梯度检查（使用 torch.autograd.gradcheck）
    def fwd(x_):
        return MyConv1dFn.apply(x_, conv.weight, conv.bias, stride, padding, dilation)

    x_gc = x.clone().requires_grad_(True)
    # gradcheck 需要输入是 tuple，并且函数需返回 double 类型
    ok = torch.autograd.gradcheck(
        lambda inp: fwd(inp),  # 只对 x 做数值检查（w,b 在模块里作为常量）
        (x_gc,),
        eps=1e-6, atol=1e-4, rtol=1e-3
    )
    print("Gradcheck on x:", ok)

    # 与 nn.Conv1d 对比（仅用于校验质量；实现未调用其前向）
    ref = nn.Conv1d(Cin, Cout, K, stride=stride, padding=padding, dilation=dilation, bias=True).to(device).double()
    with torch.no_grad():
        # 将参数拷贝过去（确保相同权重）
        ref.weight.copy_(conv.weight)
        if conv.bias is not None:
            ref.bias.copy_(conv.bias)

    y_my = conv(x)
    y_ref = ref(x.detach().clone().requires_grad_(True))
    print("Forward max abs diff:", (y_my - y_ref).abs().max().item())

    # 反向：随机损失
    loss = y_my.pow(2).mean()
    loss.backward()

    # 对比权重梯度（与 nn.Conv1d）
    ref.zero_grad(set_to_none=True)
    (y_ref.pow(2).mean()).backward()
    gw_diff = (conv.weight.grad - ref.weight.grad).abs().max().item()
    gb_diff = (conv.bias.grad - ref.bias.grad).abs().max().item() if conv.bias is not None else 0.0
    print("Grad W max abs diff:", gw_diff)
    print("Grad b max abs diff:", gb_diff)

    # 形状检查
    Lout = (Lin + 2*padding - dilation*(K-1) - 1)//stride + 1
    assert y_my.shape == (B, Cout, Lout), "Output shape mismatch"
    print("All checks passed ✓")
```
