# 题目：加性注意力（Seq2Seq 对齐）

**目标**：实现加性注意力，用于对齐 decoder 当前隐状态 $s_t$ 与 encoder 序列 $H$。

---

## 数学定义

给定 $H=\{h_1,\dots,h_L\},\, s_t$：

$$
e_{t,i} = v^\top \tanh(W_h h_i + W_s s_t),\quad
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})},\quad
c_t = \sum_i \alpha_{t,i} h_i
$$

---

## 实现要求

- `H` 形状 $(B,L,d_h)$，`s_t` 形状 $(B,d_s)$。
- 允许 padding mask（形状 $(B,L)$，`0` 为 pad）。

---

## 参考实现

```python
import torch
import torch.nn as nn

class AdditiveAttention(nn.Module):
    def __init__(self, d_h, d_s, d_attn):
        super().__init__()
        self.W_h = nn.Linear(d_h, d_attn, bias=False)
        self.W_s = nn.Linear(d_s, d_attn, bias=False)
        self.v   = nn.Linear(d_attn, 1, bias=False)

    def forward(self, H, s_t, mask=None):
        # H: (B,L,d_h), s_t:(B,d_s)
        B,L,_ = H.shape
        Wh = self.W_h(H)                    # (B,L,d_a)
        Ws = self.W_s(s_t).unsqueeze(1)     # (B,1,d_a)
        e   = self.v(torch.tanh(Wh + Ws)).squeeze(-1)  # (B,L)
        if mask is not None:
            e = e.masked_fill(mask==0, -1e9)
        alpha = torch.softmax(e, dim=-1)    # (B,L)
        c = torch.bmm(alpha.unsqueeze(1), H).squeeze(1)  # (B,d_h)
        return c, alpha

# 验证
B,L,dh,ds,da = 2,7,16,12,32
H = torch.randn(B,L,dh)
s = torch.randn(B,ds)
mask = torch.randint(0,2,(B,L))  # 0/1
attn = AdditiveAttention(dh,ds,da)
c,a = attn(H,s,mask)
print(c.shape, a.shape)
```
