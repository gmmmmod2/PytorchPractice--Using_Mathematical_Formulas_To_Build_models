# 题目：加性注意力

> 目标 实现加性注意力，用于对齐查询向量 $Q$ 与键值对 $(K,V)$。

## 数学定义

给定

- 查询序列 $Q \in \mathbb{R}^{B \times T_q \times d_q}$
- 键序列 $K \in \mathbb{R}^{B \times T_k \times d_k}$
- 值序列 $V \in \mathbb{R}^{B \times T_k \times d_v}$

加性注意力的打分函数为

$$
\text{score}(Q, K) = v^\top \tanh(W_q Q + W_k K)
$$

归一化注意力权重为

$$
\alpha = \text{softmax}(\text{score})
$$

上下文向量为

$$
\text{output} = \sum \alpha_iV_i
$$

## 额外的输入/输出规定

- Q: $(B, T_q, d_q)$
- K: $(B, T_k, d_k)$
- V: $(B, T_k, d_v)$
- output: $(B, T_q, d_v)$
- attn_weights: $(B, T_q, T_k)$

## 参考答案

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AdditiveAttention(nn.Module):
    def __init__(self, query_dim, key_dim, attn_dim, dropout=0.1,bias=False):
        """
        :param query_dim:查询向量的维度
        :param key_dim:键向量的维度
        :param attn_dim:注意力隐藏层维度
        :param dropout:dropout比率
        """
        super(AdditiveAttention, self).__init__()
        self.query_dim = query_dim
        self.key_dim = key_dim
        self.attn_dim = attn_dim

        self.W_q=nn.Linear(query_dim,attn_dim,bias=bias)
        self.W_k=nn.Linear(key_dim,attn_dim,bias=bias)
        self.v=nn.Linear(attn_dim,1,bias=bias)
        self.tanh=nn.Tanh()
        self.dropout=nn.Dropout(dropout)


    def forward(self,q,k,v,mask=None):
        """
        :param q:查询向量 [batch_size,seq_len, query_dim]
        :param k:键向量 [batch_size, seq_len, key_dim]
        :param v:值向量 [batch_size, seq_len, value_dim]
        :param mask:掩码 [batch_size, seq_len]
        :return:context: 上下文向量 [batch_size, value_dim]
                attn_weights: 注意力权重 [batch_size, seq_len]
        """
        score=self.v(self.tanh(self.W_q(q)+self.W_k(k))).squeeze(-1) #[batch_size,seq_len]
        if mask is not None:
            score=score.masked_fill(mask==0,float("-inf"))
        attn_weights=F.softmax(score,dim=-1)
        attn_weights=self.dropout(attn_weights)
        context=torch.bmm(attn_weights.unsqueeze(1),v).squeeze(1)
        return context,attn_weights


#额外变体：多头加性注意力
class MultiHeadAdditiveAttention(nn.Module):
    def __init__(self, num_heads, query_dim, key_dim, attn_dim, dropout=0.1,bias=False):
        super(MultiHeadAdditiveAttention, self).__init__()
        self.attn=nn.ModuleList(
            [AdditiveAttention(query_dim, key_dim, attn_dim, dropout,bias=bias) for _ in range(num_heads)]
        )
        self.out_proj=nn.Linear(value_dim*num_heads,value_dim,bias=bias)

    def forward(self,q,k,v,mask=None):
        """
        :param q: [batch_size,seq_len, query_dim]
        :param k: [batch_size,seq_len, key_dim]
        :param v: [batch_size,seq_len, value_dim]
        :param mask: [batch_size, seq_len]
        :return out: [batch_size,value_dim]
                attn_weights 平均注意力权重 [batch_size,seq_len]
        """
        context=[]
        attn_weights=[]
        for attn in self.attn:
            c,a=attn(q,k,v,mask=mask)
            context.append(c)
            attn_weights.append(a)
        context=torch.cat(context,dim=-1) #[batch_size, value_dim*num_heads]
        out=self.out_proj(context)
        attn_weights=torch.stack(attn_weights,dim=1) #[batch_size, num_heads, seq_len]
        attn_weights=attn_weights.mean(dim=1)
        return out,attn_weights

if __name__=='__main__':
    # 参数设置
    batch_size = 32
    seq_len = 10
    query_dim = 64
    key_dim = 64
    value_dim = 128
    attn_dim = 256
    num_heads = 2

    # 创建加性注意力层
    additive_attn = AdditiveAttention(query_dim, key_dim, attn_dim)

    # 创建输入数据
    query = torch.randn(batch_size, seq_len,query_dim)
    keys = torch.randn(batch_size, seq_len, key_dim)
    values = torch.randn(batch_size, seq_len, value_dim)

    # 前向传播
    context, attn_weights = additive_attn(query, keys, values)

    print("单头加性注意力")
    print(f"Context shape: {context.shape}")  # [32, 128]
    print(f"Attention weights shape: {attn_weights.shape}")  # [32, 10]
    print("-"*60)
    #创建多头加性注意力层
    attn=MultiHeadAdditiveAttention(num_heads, query_dim, key_dim, attn_dim)
    context, attn_weights = attn(query, keys, values)
    print("多头加性注意力")
    print(f"Context shape: {context.shape}")  # [32, 128]
    print(f"Attention weights shape: {attn_weights.shape}")  # [32, 10]
```
