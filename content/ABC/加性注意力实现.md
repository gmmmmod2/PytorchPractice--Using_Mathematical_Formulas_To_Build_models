# 题目：加性注意力（Seq2Seq 对齐）

**目标**：实现加性注意力，用于对齐 decoder 当前隐状态 $s_t$ 与 encoder 序列 $H$。

## 数学定义

给定 $H=\{h_1,\dots,h_L\},\, s_t$：

$$
e_{t,i} = v^\top \tanh(W_h h_i + W_s s_t),\quad
\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_j \exp(e_{t,j})},\quad
c_t = \sum_i \alpha_{t,i} h_i
$$

## 输入 / 输出

- `H` 形状 $(B,L,d_h)$，`s_t` 形状 $(B,d_s)$。
- `W_h` 和 `W_s` 都将 `H` 和 `s_t` 映射到维度 `d_attn`
- 允许 padding mask(形状 $(B,L)$，`0` 为 pad), 对无效位置加上一个足够大的负数。

## 参考实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class AdditiveAttention(nn.Module):
    def __init__(self, d_h: int, d_s: int, d_attn: int):
        super().__init__()
        self.W_h = nn.Parameter(torch.empty(d_attn, d_h))
        self.W_s = nn.Parameter(torch.empty(d_attn, d_s))
        self.v   = nn.Parameter(torch.empty(d_attn))

        self.act = nn.Tanh()
        self.reset_parameters()

    def reset_parameters(self):
        # Xavier/Glorot 初始化：适合 tanh
        nn.init.xavier_uniform_(self.W_h)
        nn.init.xavier_uniform_(self.W_s)
        # v 当作列向量，同样用 xavier（实现上需要 2D 张量）
        nn.init.xavier_uniform_(self.v.unsqueeze(0))  # (1, d_attn)

    def forward(self, H: torch.Tensor, s_t: torch.Tensor, padding_mask: torch.Tensor | None = None):
        Wh = H @ self.W_h.T                   # (B,L,d_attn)
        Ws = (s_t @ self.W_s.T).unsqueeze(1)  # (B,1,d_attn)
        e  = self.act(Wh + Ws)                # (B, L, d_attn)
        if padding_mask is not None:
            e = e.masked_fill(~padding_mask, float('-inf'))

        alpha = F.softmax(e, dim=-1)                    # (B, L)
        c = torch.bmm(alpha.unsqueeze(1), H).squeeze(1) # (B, d_attn)
        return c, alpha
```
