# 完整复现 **多头自注意力（Multi-Head Self-Attention）**（支持因果与填充 mask）

> 目标：仅根据下面的数学定义，用 PyTorch **手写**一个可复用的 `MultiHeadSelfAttention` 模块，支持：
>
> - 多头拆分与合并
> - `attention_dropout` 与 `proj_dropout`
> - **因果（causal）mask** 与 **填充（padding）mask**（二者可叠加）
> - 保持与标准实现一致的数值尺度（缩放点积注意力）

## 1. 数学定义

设输入序列为 $X \in \mathbb{R}^{B \times T \times d_{\text{model}}}$，头数为 $h$，每头维度 $d_k = d_v = \frac{d_{\text{model}}}{h}$。对同一输入做线性映射得到 $Q,K,V$：

$$
Q = X W_Q + \mathbf{1} b_Q^\top,\quad
K = X W_K + \mathbf{1} b_K^\top,\quad
V = X W_V + \mathbf{1} b_V^\top,
$$

其中 $W_Q,W_K,W_V \in \mathbb{R}^{d_{\text{model}}\times (d_k h)}$，$b_Q,b_K,b_V \in \mathbb{R}^{d_k h}$。将最后一维 reshape 为 $(h,d_k)$ 并交换维度得到按头展开的表示：

$$
Q \to Q^{(h)} \in \mathbb{R}^{B \times h \times T \times d_k},\quad
K^{(h)} \in \mathbb{R}^{B \times h \times T \times d_k},\quad
V^{(h)} \in \mathbb{R}^{B \times h \times T \times d_k}.
$$

单头注意力的得分与权重为：

$$
S = \frac{Q^{(h)} {K^{(h)}}^\top}{\sqrt{d_k}} \in \mathbb{R}^{B \times h \times T \times T}.
$$

若给定 **因果 mask**（上三角位置为 $-\infty$ 屏蔽）$M_{\text{causal}}$ 与 **填充 mask**（在被填充的键位置加 $-\infty$）$M_{\text{pad}}$，则总 mask：

$$
M = M_{\text{causal}} + M_{\text{pad}},
$$

并在 softmax 前加到分数上：

$$
\tilde{S} = S + M,\qquad
A = \operatorname{Softmax}(\tilde{S}) \in \mathbb{R}^{B \times h \times T \times T}.
$$

注意力输出：

$$
O^{(h)} = A \, V^{(h)} \in \mathbb{R}^{B \times h \times T \times d_v}.
$$

拼接各头并线性投影：

$$
O = \operatorname{Concat}_h(O^{(h)}) W_O + \mathbf{1} b_O^\top,\quad
W_O \in \mathbb{R}^{(h d_v)\times d_{\text{model}}},\ b_O \in \mathbb{R}^{d_{\text{model}}}.
$$

> 约定：dropout 仅作用于 $A$（即 $O^{(h)}= \operatorname{Dropout}(A) V^{(h)}$）与最终投影 $O$（`proj_dropout`）。

### 输入与 mask 约定

- `x`: 形状 $(B,T,d_model)$。
- `attn_mask`: 可选。形状之一：
  - $(B,1,1,T)$：**键位置**的可达性（1=保留, 0=屏蔽）；内部会转换为 $0$ 或 $-\infty$ 的 **加性 mask**。
  - $(B,1,T,T)$：**查询到键**的逐对可达性。
- `causal`: `True/False`。为 `True` 时自动构造上三角 $-\infty$ mask。

## 2. 你需要完成的内容

1. **实现**一个 `MultiHeadSelfAttention`（PyTorch `nn.Module`），严格遵守上面公式。
2. 正确处理两类 mask，并支持两者叠加。

# 参考答案

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model: int, num_heads: int, attn_dropout: float, proj_dropout: float):
        super().__init__()
        assert d_model % num_heads == 0,"d_model 必须能被 num_heads 整除"

        self.d_model = d_model
        self.h = num_heads
        self.d_k = d_model // num_heads
        self.qkv = nn.Linear(d_model, 3*d_model)  # 更高效的方法, 一次性完成三次计算
        self.linear = nn.Linear(d_model, d_model) # 线性投影

        self.attn_drop = nn.Dropout(attn_dropout)
        self.proj_drop = nn.Dropout(proj_dropout)

    def _shapeToHeads(self, x: torch.Tensor) -> torch.Tensor:
        # (B,T,h*d_k) -> (B,h,T,d_k)
        B,T,_ = x.shape
        x = x.reshape(B,T,self.h,self.d_k).permute(0,2,1,3)
        return x

    @staticmethod
    def _makeCausalMask(B, T, device):
        mask = torch.full((B, 1, T, T), -1e9, device=device) # 上三角(不含对角)位置置 -inf，用于加性 mask
        idx = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool)) # 允许下三角(含对角)：用 0 覆盖
        mask[:, :, idx] = 0.0
        return mask

    @staticmethod
    def _additiveMask(attn_mask: torch.Tensor) -> torch.Tensor:
        assert attn_mask.dim() == 4, f"mask 应当是四维张量, 得到 {attn_mask.dim()} 维"
        B, H, A, T = attn_mask.shape
        assert H == 1, f"mask 的第二个维度应当是 1, 得到 {H}"
        if attn_mask.dtype != torch.bool:
            attn_mask = attn_mask > 0

        if A == 1:  # (B,1,1,T)
            attn_mask = attn_mask.expand(B, 1, T, T)
        else:
            assert A == T, f"期望 (B,1,T,T)，得到 {attn_mask.shape}"

        add_mask = torch.zeros((B, 1, T, T), dtype=torch.float32, device=attn_mask.device)
        add_mask = add_mask.masked_fill(~attn_mask, -1e9) # (B,1,T,T)
        return add_mask

    def forward(self, x: torch.Tensor, attn_mask: torch.Tensor|None = None, causal: bool = True,) -> torch.Tensor:
        """
        x: (B,T,d_model)
        attn_mask: (B,1,1,T) 或 (B,1,T,T)
        """
        B, T, _ = x.shape

        qkv = self.qkv(x)            # (B,T,d_model * 3)
        Q,K,V = qkv.chunk(3, dim=-1) # 3 * (B,T,d_model) = 3 * (B,T,h*d_k)

        Q = self._shapeToHeads(Q)  # (B,h,T,d_k)
        K = self._shapeToHeads(K)  # (B,h,T,d_k)
        V = self._shapeToHeads(V)  # (B,h,T,d_k)

        scores = torch.matmul(Q, K.transpose(-2,-1))/(self.d_k ** 0.5) # (B,h,T,T)
        add_masks = []
        if attn_mask is not None:
            attn_mask = self._additiveMask(attn_mask)
            add_masks.append(attn_mask)
        if causal:
            causal_mask = self._makeCausalMask(B,T,x.device)
            add_masks.append(causal_mask)
        if add_masks:
            scores = scores + sum(add_masks) # (B,h,T,T)

        scores = F.softmax(scores, dim=-1)
        scores = self.attn_drop(scores) # (B,h,T,T)

        # 计算注意力输出
        out = torch.matmul(scores,V)                               # (B,h,T,d_k)
        out = out.permute(0, 2, 1, 3).reshape(B, T, self.d_model)  # (B,h,T,d_k) -> (B,T,h,d_k) -> (B,T,d_model)

        out = self.proj_drop(self.linear(out))
        return out
```
