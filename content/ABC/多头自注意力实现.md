# 题目（ABC 级别·NLP 常用模块）：从公式复现 **多头自注意力（Multi-Head Self-Attention）**（支持因果与填充 mask）

> 目标：仅根据下面的数学定义，用 PyTorch **手写**一个可复用的 `MultiHeadSelfAttention` 模块，支持：
>
> - 多头拆分与合并
> - `attention_dropout` 与 `proj_dropout`
> - **因果（causal）mask** 与 **填充（padding）mask**（二者可叠加）
> - 保持与标准实现一致的数值尺度（缩放点积注意力）

## 1. 数学定义

设输入序列为 $X \in \mathbb{R}^{B \times T \times d_{\text{model}}}$，头数为 $h$，每头维度 $d_k = d_v = \frac{d_{\text{model}}}{h}$。对同一输入做线性映射得到 $Q,K,V$：

$$
Q = X W_Q + \mathbf{1} b_Q^\top,\quad
K = X W_K + \mathbf{1} b_K^\top,\quad
V = X W_V + \mathbf{1} b_V^\top,
$$

其中 $W_Q,W_K,W_V \in \mathbb{R}^{d_{\text{model}}\times (d_k h)}$，$b_Q,b_K,b_V \in \mathbb{R}^{d_k h}$。将最后一维 reshape 为 $(h,d_k)$ 并交换维度得到按头展开的表示：

$$
Q \to Q^{(h)} \in \mathbb{R}^{B \times h \times T \times d_k},\quad
K^{(h)} \in \mathbb{R}^{B \times h \times T \times d_k},\quad
V^{(h)} \in \mathbb{R}^{B \times h \times T \times d_k}.
$$

单头注意力的得分与权重为：

$$
S = \frac{Q^{(h)} {K^{(h)}}^\top}{\sqrt{d_k}} \in \mathbb{R}^{B \times h \times T \times T}.
$$

若给定 **因果 mask**（上三角位置为 $-\infty$ 屏蔽）$M_{\text{causal}}$ 与 **填充 mask**（在被填充的键位置加 $-\infty$）$M_{\text{pad}}$，则总 mask：

$$
M = M_{\text{causal}} + M_{\text{pad}},
$$

并在 softmax 前加到分数上：

$$
\tilde{S} = S + M,\qquad
A = \operatorname{Softmax}(\tilde{S}) \in \mathbb{R}^{B \times h \times T \times T}.
$$

注意力输出：

$$
O^{(h)} = A \, V^{(h)} \in \mathbb{R}^{B \times h \times T \times d_v}.
$$

拼接各头并线性投影：

$$
O = \operatorname{Concat}_h(O^{(h)}) W_O + \mathbf{1} b_O^\top,\quad
W_O \in \mathbb{R}^{(h d_v)\times d_{\text{model}}},\ b_O \in \mathbb{R}^{d_{\text{model}}}.
$$

> 约定：dropout 仅作用于 $A$（即 $O^{(h)}= \operatorname{Dropout}(A) V^{(h)}$）与最终投影 $O$（`proj_dropout`）。

### 输入与 mask 约定

- `x`: 形状 $(B,T,d_model)$。
- `attn_mask`: 可选。形状之一：
  - $(B,1,1,T)$：**键位置**的可达性（1=保留, 0=屏蔽）；内部会转换为 $0$ 或 $-\infty$ 的 **加性 mask**。
  - $(B,1,T,T)$：**查询到键**的逐对可达性。
- `causal`: `True/False`。为 `True` 时自动构造上三角 $-\infty$ mask。

## 2. 你需要完成的内容

1. **实现**一个 `MultiHeadSelfAttention`（PyTorch `nn.Module`），严格遵守上面公式。
2. 正确处理两类 mask，并支持两者叠加。
3. 提供**最小可运行用例**与**数值/形状检查**：
   - 比较 `causal=True` 与 `False` 时，上三角区域注意力权重差异。
   - 在存在 `padding mask` 时，被屏蔽 token 的注意力质量应趋近 $0$。

# 参考答案

> 下面给出从公式到代码的完整复现（包含注释与测试）。

## 1) 关键实现要点（从式到算子）

- 线性层一次性产生 $Q,K,V$：利用单个 `nn.Linear(d_model, 3*h*d_k, bias=True)`，再 `chunk(3, dim=-1)`。
- 维度变换：`(B,T,h,d_k) -> (B,h,T,d_k)` 用 `reshape` + `permute`。
- 缩放：`scores = (Q @ K.transpose(-2,-1)) / math.sqrt(d_k)`。
- mask：使用**加性 mask**（加上非常大的负数）。对两种输入形状进行广播到 `(B,h,T,T)`。
- dropout：`attn = dropout(attn)` 与 `out = proj_dropout(W_O(out))`。

## 2) 代码

```python
import math
from typing import Optional
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadSelfAttention(nn.Module):
    def __init__(
        self,
        d_model: int,
        num_heads: int,
        attention_dropout: float = 0.0,
        proj_dropout: float = 0.0,
    ):
        super().__init__()
        assert d_model % num_heads == 0, "d_model 必须能被 num_heads 整除"
        self.d_model = d_model
        self.h = num_heads
        self.d_k = d_model // num_heads

        # 同时生成 Q,K,V
        self.qkv = nn.Linear(d_model, 3 * d_model, bias=True)
        # 输出投影
        self.proj = nn.Linear(d_model, d_model, bias=True)

        self.attn_drop = nn.Dropout(attention_dropout)
        self.proj_drop = nn.Dropout(proj_dropout)

    def _shape_to_heads(self, x: torch.Tensor):
        # (B, T, d_model) -> (B, h, T, d_k)
        B, T, _ = x.shape
        x = x.reshape(B, T, self.h, self.d_k).permute(0, 2, 1, 3)
        return x

    @staticmethod
    def _make_causal_mask(B, T, device):
        # 上三角（不含对角）位置置 -inf，用于加性 mask
        mask = torch.full((B, 1, T, T), float("-inf"), device=device)
        # 允许下三角（含对角）：用 0 覆盖
        idx = torch.tril(torch.ones(T, T, device=device, dtype=torch.bool))
        mask[:, :, idx] = 0.0
        return mask

    @staticmethod
    def _as_additive_mask(attn_mask: torch.Tensor, h: int) -> torch.Tensor:
        """
        将用户提供的二进制/布尔 mask 转为加性 mask（0 或 -inf）。
        支持形状：
          - (B,1,1,T): 键位置可用性（1=keep,0=mask）
          - (B,1,T,T): 查询到键的可用性
        返回形状：(B,h,T,T) 以便广播到各头。
        """
        B = attn_mask.shape[0]
        if attn_mask.dtype != torch.float32 and attn_mask.dtype != torch.float64:
            attn_mask = attn_mask.float()

        if attn_mask.dim() == 4 and attn_mask.shape[2] == 1:  # (B,1,1,T)
            # 先把 (B,1,1,T) -> (B,1,T,T) 使每个查询看到相同键可用性
            attn_mask = attn_mask.expand(B, 1, attn_mask.shape[-1], attn_mask.shape[-1])
        assert attn_mask.shape[1] == 1, "mask 第二维应为1以便广播到各头"
        # 二进制 -> 加性：1->0, 0->-inf
        add_mask = (1.0 - attn_mask) * float("-inf")
        add_mask = add_mask.expand(B, h, add_mask.shape[-2], add_mask.shape[-1])
        return add_mask

    def forward(
        self,
        x: torch.Tensor,                         # (B,T,d_model)
        attn_mask: Optional[torch.Tensor] = None,# (B,1,1,T) 或 (B,1,T,T)
        causal: bool = False,
        need_weights: bool = False,              # 若为 True，返回平均后的注意力权重
    ):
        B, T, _ = x.shape
        qkv = self.qkv(x)                        # (B,T,3*d_model)
        q, k, v = qkv.chunk(3, dim=-1)

        q = self._shape_to_heads(q)              # (B,h,T,d_k)
        k = self._shape_to_heads(k)              # (B,h,T,d_k)
        v = self._shape_to_heads(v)              # (B,h,T,d_k)

        # 注意力分数
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # (B,h,T,T)

        # 构造/叠加 mask（加性形式）
        add_masks = []
        if causal:
            add_masks.append(self._make_causal_mask(B, T, x.device))
        if attn_mask is not None:
            add_masks.append(self._as_additive_mask(attn_mask.to(x.device), self.h))
        if add_masks:
            scores = scores + sum(add_masks)

        # 注意力权重
        attn = F.softmax(scores, dim=-1)         # (B,h,T,T)
        attn = self.attn_drop(attn)

        # 加权求和
        out = torch.matmul(attn, v)              # (B,h,T,d_k)

        # 合并各头
        out = out.permute(0, 2, 1, 3).reshape(B, T, self.d_model)  # (B,T,d_model)

        # 输出投影
        out = self.proj_drop(self.proj(out))     # (B,T,d_model)

        if need_weights:
            # 返回对头求平均的注意力（B,T,T）
            attn_mean = attn.mean(dim=1)
            return out, attn_mean
        return out
```
