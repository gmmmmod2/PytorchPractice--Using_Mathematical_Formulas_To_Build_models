# 题目：实现 Scaled Dot-Product Attention（单头版，支持两类 mask）

## 目标

根据公式实现“缩放点积注意力”（单头）。要求同时支持：

- **padding mask**：把 padding 的位置权重置为 0；
- **causal mask**：自回归时，禁止看到未来位置信息。

## 公式

给定查询 $Q \in \mathbb{R}^{T_q \times d}$，键 $K \in \mathbb{R}^{T_k \times d}$，值 $V \in \mathbb{R}^{T_k \times d_v}$：

$$
\text{Attention}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d}} + M\right)V
$$

其中 $M$ 是 mask，在被屏蔽处为 $-\infty$，其余为 0。

## 输入/输出规定

- 输入：`Q, K, V` 形状为 `(B, T, d)`（为简便设 `d_v = d`），可选：
  - `padding_mask`: `(B, 1, T)`，True/1 表示“该位置是 padding，需要屏蔽”；
  - `causal`: bool。
- 输出：`Y` 形状 `(B, T, d)` 以及注意力权重 `A` 形状 `(B, T, T)`。

## 其它要求

- 需考虑数值稳定性（softmax 前减最大值）。
- 允许传入 `dropout_p`（对注意力权重做 dropout）。
- 不允许使用 `torch.nn.MultiheadAttention` 等现成实现。

## 参考答案

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d, dropout_p=0.0):
        super().__init__()
        self.scale = d ** 0.5
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, Q, K, V, padding_mask=None, causal=False):
        """
        Q, K, V: (B, T, d)
        padding_mask: (B, 1, T)  True=mask  (屏蔽键/值的时间步)
        causal: 是否使用因果mask (禁止看到未来)
        """
        B, T, d = Q.shape
        # (B, T, T)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # 数值稳定处理：减去每行最大值
        scores = scores - scores.amax(dim=-1, keepdim=True)

        # padding mask：屏蔽被 mask 的 key 位置
        if padding_mask is not None:
            # broadcast 到 (B, T, T)
            scores = scores.masked_fill(padding_mask.expand(-1, T, -1), float('-inf'))

        # causal mask：屏蔽未来信息（上三角）
        if causal:
            causal_mask = torch.triu(torch.ones(T, T, device=scores.device, dtype=torch.bool), diagonal=1)
            scores = scores.masked_fill(causal_mask.unsqueeze(0), float('-inf'))

        A = F.softmax(scores, dim=-1)
        A = self.dropout(A)
        Y = torch.matmul(A, V)
        return Y, A
```
