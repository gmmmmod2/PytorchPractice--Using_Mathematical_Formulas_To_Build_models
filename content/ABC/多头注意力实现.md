# 题目：Multi-Head Self-Attention（不含位置前馈）

**目标**：根据多头注意力公式实现自注意力层（不含 FFN），支持可选 `bias` 与 `mask`。

## 数学定义

将 $d_\text{model}$ 切分为 $h$ 个头，每头维度 $d_h = d_\text{model}/h$：

$$
Q = XW_Q,\quad K = XW_K,\quad V = XW_V,\quad W_Q,W_K,W_V \in \mathbb{R}^{d_\text{model}\times d_\text{model}}
$$

分头并做注意力：

$$
\mathrm{head}_i = \mathrm{Attention}(Q_i,K_i,V_i)
$$

拼接并线性：

$$
\mathrm{MHA}(X)=\mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W_O
$$

## 实现要求

- 使用点积缩放题目的 `ScaledDotProductAttention` 并按照当前条件做出改变。
- 处理 `view/transpose/contiguous` 以实现 $(B,L,h,d_h)$ 形状。
- 输出形状 $(B,L,d_\text{model})$。

## 参考实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaleAttention(nn.Module):
    def __init__(self, d: int, dropout: float):
        super().__init__()

        self.scale = d ** 0.5
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,
                padding_mask: torch.Tensor = None, causal: bool = False):
        """
        Q, K, V: (B, h, L, d_h)
        padding_mask: (B, T)
        True: mask
        """
        B, h, L, d_h = Q.shape
        att_scores = torch.matmul(Q, K.transpose(-2,-1)) / self.scale # (B,h,L,L)

        if padding_mask is not None:
            mask_bool = padding_mask.to(dtype=torch.bool)
            key_mask = mask_bool.unsqueeze(1).unsqueeze(2)  #(B, T) -> (B,1,1,L)
            att_scores = att_scores.masked_fill(key_mask, float("-inf"))  # (B,h,L,L)
        if causal:
            causal_mask = torch.triu(torch.ones((L, L), dtype=torch.bool, device=att_scores.device), diagonal=1)
            att_scores = att_scores.masked_fill(causal_mask.unsqueeze(0).unsqueeze(0), float("-inf"))

        att_scores = att_scores - att_scores.amax(dim=-1, keepdim=True) # 数值稳定, 不变分布只平移中心, 降低exp溢出风险

        att_weights = F.softmax(att_scores, dim=-1)
        att_weights = self.dropout(att_weights) # 对注意力权重做 dropout
        context = torch.matmul(att_weights, V)  # context: (B, h, L, d_h)
        return context, att_weights

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.0, bias=True):
        super().__init__()
        assert d_model % num_heads == 0
        self.h = num_heads
        self.d_h = d_model // num_heads
        self.qkv = nn.Linear(d_model, 3*d_model, bias=bias)
        self.o = nn.Linear(d_model, d_model, bias=bias)
        self.attn = ScaleAttention(self.d_h, dropout=dropout)

    def _split(self, x):  # (B,L,d) -> (B,h,L,d_h)
        B,L,_=x.shape
        x = x.view(B, L, self.h, self.d_h).transpose(1, 2)  # (B,h,L,d_h)
        return x

    def _merge(self, x):  # (B,h,L,d_h) -> (B,L,d)
        x = x.transpose(1,2).contiguous()
        B,L,_,_=x.shape
        return x.view(B,L,self.h*self.d_h)

    def forward(self, x, mask=None, causal=False):
        B,L,d = x.shape
        q,k,v = self.qkv(x).chunk(3, dim=-1)
        q,k,v = map(self._split, (q,k,v))  # (B,h,L,d_h)
        # 逐头计算
        context, attn = self.attn(q, k, v, mask, causal=causal)
        out = self.o(self._merge(context))
        return out, attn  # (B,L,d), (B,h,L,L)
```
