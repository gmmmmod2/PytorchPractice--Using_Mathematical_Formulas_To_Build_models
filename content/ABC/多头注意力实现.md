# 题目：Multi-Head Self-Attention（不含位置前馈）

**目标**：根据多头注意力公式实现自注意力层（不含 FFN），支持可选 `bias` 与 `mask`。

## 数学定义

将 $d_\text{model}$ 切分为 $h$ 个头，每头维度 $d_h = d_\text{model}/h$：

$$
Q = XW_Q,\quad K = XW_K,\quad V = XW_V,\quad W_Q,W_K,W_V \in \mathbb{R}^{d_\text{model}\times d_\text{model}}
$$

分头并做注意力：

$$
\mathrm{head}_i = \mathrm{Attention}(Q_i,K_i,V_i)
$$

拼接并线性：

$$
\mathrm{MHA}(X)=\mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W_O
$$

## 实现要求

- 使用点积缩放题目的 `ScaledDotProductAttention` 并按照当前条件做出改变。
- 处理 `view/transpose/contiguous` 以实现 $(B,L,h,d_h)$ 形状。
- 输出形状 $(B,L,d_\text{model})$。

## 参考实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaleAttention(nn.Module):
    def __init__(self,d: int,dropout: float,device=None):
        super(ScaleAttention,self).__init__()
        self.d=d
        self.dropout=nn.Dropout(dropout)
        self.scale=d**0.5
        self.device=device

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,
                padding_mask: torch.Tensor = None, causal: bool = False):
        """
        :param Q: [B,h,L,d_h]
        :param K: [B,h,L,d_h]
        :param V: [B,h,L,d_h]
        :param padding_mask: [B,L]，如果没有掩码，就为None
        :param causal: 是否要做因果掩码

        return:
        context: [B,h,L,d_h]
        attention_weights: [B,d_h,L,L]
        """
        L=Q.shape[2]
        #确保在同一个device
        Q=Q.to(self.device)
        K=K.to(self.device)
        V=V.to(self.device)


        attention_score=torch.matmul(Q,K.transpose(-1,-2))/self.scale #[B,d_h,L,L]

        #掩码注意力
        if padding_mask is not None:
            padding_mask = padding_mask.to(self.device)
            padding_mask=padding_mask.to(dtype=torch.bool)
            padding_mask=padding_mask.unsqueeze(1).unsqueeze(2) #[B,1,1,L]
            attention_score=attention_score.masked_fill(padding_mask,float('-inf'))
        #因果掩码(在训练时防止模型"作弊"看到未来信息)
        if causal:
            causal_mask=torch.triu(torch.ones((L,L),device=self.device,dtype=torch.bool),diagonal=1)
            causal_mask=causal_mask.unsqueeze(0).unsqueeze(0) #[1,1,L,L]
            attention_score=attention_score.masked_fill(causal_mask,float('-inf'))

        attention_score=attention_score-attention_score.amax(dim=-1,keepdim=True) #防止溢出
        attention_weights=F.softmax(attention_score,dim=-1) #[B,d_h,L,L]
        attention_weights=self.dropout(attention_weights)
        context=torch.matmul(attention_weights,V) #[B,d_h,L,d]
        return context,attention_weights

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.2, bias=True,device=None):
        super(MultiHeadSelfAttention, self).__init__()
        assert d_model % num_heads==0
        self.d_model=d_model
        self.h=num_heads
        self.d_h=d_model//num_heads
        self.qkv=nn.Linear(d_model,d_model*3,bias=bias)
        self.out=nn.Linear(d_model,d_model,bias=bias)
        self.attn=ScaleAttention(self.d_h,dropout,device=device)

    def _split(self,x):
        """
        :param x: [B,L,d]
        :return: x_split: [B,h,L,d_h]
        """
        B,L=x.shape[:2]
        x_split=x.view(B,L,self.h,self.d_h).transpose(1,2)
        return x_split

    def _concat(self,x):
        """
        :param x: [B,h,L,d_h]
        :return: x_concat: [B,L,d]
        """
        B=x.shape[0]
        L=x.shape[2]
        x=x.transpose(1,2).contiguous()
        x_concat=x.view(B,L,self.d_model)
        return x_concat

    def forward(self,x,padding_mask=None,causal=False):
        """
        :param x: [B,L,d]
        :return: output [B,L,d]
                 attention_weights [B,d_h,L,L]
        """
        q,k,v=self.qkv(x).chunk(3,dim=-1)
        q,k,v=map(self._split,(q,k,v))
        context,attention_weights=self.attn(q,k,v,padding_mask=padding_mask,causal=causal)
        context=self._concat(context)
        output=self.out(context)
        return output,attention_weights

if __name__=='__main__':
    B,L,d_model,num_heads=2,4,8,2
    x=torch.randn(B,L,d_model)
    attn=MultiHeadSelfAttention(d_model,num_heads)
    output,attention_weights=attn(x)
    print(x.shape) #[2,4,8]
    print(attention_weights.shape) #[2,2,4,4]
    print(output.shape) #[2,4,8]
```
