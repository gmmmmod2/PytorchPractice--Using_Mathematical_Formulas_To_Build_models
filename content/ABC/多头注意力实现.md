# 题目：Multi-Head Self-Attention（不含位置前馈）

**目标**：根据多头注意力公式实现自注意力层（不含 FFN），支持可选 `bias` 与 `mask`。

---

## 数学定义

将 $d_\text{model}$ 切分为 $h$ 个头，每头维度 $d_h = d_\text{model}/h$：

$$
Q = XW_Q,\quad K = XW_K,\quad V = XW_V,\quad W_Q,W_K,W_V \in \mathbb{R}^{d_\text{model}\times d_\text{model}}
$$

分头并做注意力：

$$
\mathrm{head}_i = \mathrm{Attention}(Q_i,K_i,V_i)
$$

拼接并线性：

$$
\mathrm{MHA}(X)=\mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W_O
$$

---

## 实现要求

- 使用点积缩放题目的 `ScaledDotProductAttention`。
- 处理 `view/transpose/contiguous` 以实现 $(B,L,h,d_h)$ 形状。
- 输出形状 $(B,L,d_\text{model})$。

---

## 参考实现

```python
import torch
import torch.nn as nn

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.0, bias=True):
        super().__init__()
        assert d_model % num_heads == 0
        self.h = num_heads
        self.d_h = d_model // num_heads
        self.qkv = nn.Linear(d_model, 3*d_model, bias=bias)
        self.o = nn.Linear(d_model, d_model, bias=bias)
        self.attn = ScaledDotProductAttention(dropout=dropout)

    def _split(self, x):  # (B,L,d) -> (B,h,L,d_h)
        B,L,_=x.shape
        x = x.view(B, L, self.h, self.d_h).transpose(1,2)  # (B,h,L,d_h)
        return x

    def _merge(self, x):  # (B,h,L,d_h) -> (B,L,d)
        x = x.transpose(1,2).contiguous()
        B,L,_,_=x.shape
        return x.view(B,L,self.h*self.d_h)

    def forward(self, x, mask=None):
        B,L,d = x.shape
        q,k,v = self.qkv(x).chunk(3, dim=-1)
        q,k,v = map(self._split, (q,k,v))  # (B,h,L,d_h)
        if mask is not None:
            mask = mask.unsqueeze(1)  # (B,1,L,L)
        # 逐头计算
        context, attn = self.attn(q, k, v, mask)
        out = self.o(self._merge(context))
        return out, attn  # (B,L,d), (B,h,L,L)

# 验证
B,L,d,h = 2,6,32,4
x = torch.randn(B,L,d)
mha = MultiHeadSelfAttention(d,h,dropout=0.1)
out, attn = mha(x, mask=torch.triu(torch.ones(L,L)*-1e9,1).unsqueeze(0))
print(out.shape, attn.shape)
```
