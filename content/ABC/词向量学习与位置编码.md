# 题目：可学习词向量 + 正弦/余弦位置编码（Transformer 版）

**目标**：根据公式实现将离散 token id 映射为向量，并叠加固定式正弦/余弦位置编码。  
**输入/输出**：输入形状 $(B, L)$ 的 `token_ids`，输出形状 $(B, L, d)$ 的 `X$。

## 数学定义

词嵌入矩阵 $E \in \mathbb{R}^{V \times d}$，对每个 token $t$，其向量为

$$
\mathrm{emb}(t) = E[t]
$$

位置编码（$0 \le p < L,\, 0 \le i < d/2$）（其中p表示位置索引，i表示维度索引）：

$$
\mathrm{PE}(p, 2i) = \sin\!\left(\frac{p}{10000^{2i/d}}\right),\quad
\mathrm{PE}(p, 2i+1) = \cos\!\left(\frac{p}{10000^{2i/d}}\right)
$$

最终表示：

$$
X_{b,p,:} = \mathrm{emb}(t_{b,p}) + \mathrm{PE}(p,:)
$$

## 实现要求

- 使用 `torch.nn.Embedding` 实现 $E$。
- 写一个 `build_sinusoidal_posenc(L, d, device)` 生成 $\mathrm{PE}$。
- 模块支持 `dropout`。

## 参考实现

```python
import torch
import torch.nn as nn
import math

def build_sinusoidal_posenc(L,d,device=None):
    """
    :param L: token_id的长度
    :param d: embedding的维度
    :param device: 设备
    :return: PE [L,d]
    """
    p=torch.arange(0,L,device=device).unsqueeze(1)
    x=torch.exp(torch.arange(0,d,2,device=device,dtype=torch.float32)*d*math.log(1000))
    PE=torch.zeros(L,d,device=device)
    PE[:,0::2]=torch.sin(x*p)
    PE[:,1::2]=torch.cos(x*p)
    return PE

class TokenEmbeddingWithPos(nn.Module):
    def __init__(self,L,d,drop=0.1,max_len=512,device=None):
        super(TokenEmbeddingWithPos,self).__init__()
        self.L=L
        self.d=d
        self.device=device
        self.embedding=nn.Embedding(L,d)
        self.register_buffer("PE",build_sinusoidal_posenc(max_len,d,device))
        self.dropout=nn.Dropout(drop)

    def forward(self,x):
        """
        :param x: token_id [batch_size, L]
        :return: x [batch_size, L, d]
        """
        b,l=x.shape
        x=self.embedding(x) #[batch_size,L,d]
        x=x+self.PE[:l].unsqueeze(0)
        x=self.dropout(x)
        return x

if __name__=='__main__':
    B, L, V, d = 2, 5, 100, 16
    m = TokenEmbeddingWithPos(V, d)
    tok = torch.randint(0, V, (B, L))
    out = m(tok)
    print(out.shape)  # (2,5,16)
```
