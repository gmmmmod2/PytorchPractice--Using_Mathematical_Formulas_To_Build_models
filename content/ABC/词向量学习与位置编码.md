# 题目：可学习词向量 + 正弦/余弦位置编码（Transformer 版）

**目标**：根据公式实现将离散 token id 映射为向量，并叠加固定式正弦/余弦位置编码。  
**输入/输出**：输入形状 $(B, L)$ 的 `token_ids`，输出形状 $(B, L, d)$ 的 `X$。

---

## 数学定义

1. 词嵌入矩阵 $E \in \mathbb{R}^{V \times d}$，对每个 token $t$，其向量为

   $$
   \mathrm{emb}(t) = E[t]
   $$

2. 位置编码（$0 \le p < L,\, 0 \le i < d/2$）：

   $$
   \mathrm{PE}(p, 2i) = \sin\!\left(\frac{p}{10000^{2i/d}}\right),\quad
   \mathrm{PE}(p, 2i+1) = \cos\!\left(\frac{p}{10000^{2i/d}}\right)
   $$

3. 最终表示：
   $$
   X_{b,p,:} = \mathrm{emb}(t_{b,p}) + \mathrm{PE}(p,:)
   $$

---

## 实现要求

- 使用 `torch.nn.Embedding` 实现 $E$。
- 写一个 `build_sinusoidal_posenc(L, d, device)` 生成 $\mathrm{PE}$。
- 模块支持 `dropout`。

---

## 参考实现

```python
import math
import torch
import torch.nn as nn

def build_sinusoidal_posenc(L, d, device=None):
    pos = torch.arange(L, dtype=torch.float32, device=device).unsqueeze(1)  # (L,1)
    i = torch.arange(d, dtype=torch.float32, device=device).unsqueeze(0)    # (1,d)
    # 按偶/奇位置分频率
    div = torch.exp(-(torch.arange(0, d, 2, device=device, dtype=torch.float32) * (math.log(10000.0) / d)))
    pe = torch.zeros(L, d, device=device)
    pe[:, 0::2] = torch.sin(pos * div)
    pe[:, 1::2] = torch.cos(pos * div)
    return pe  # (L, d)

class TokenEmbeddingWithPos(nn.Module):
    def __init__(self, vocab_size, d_model, max_len=512, dropout=0.1):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_model)
        self.register_buffer("pe", build_sinusoidal_posenc(max_len, d_model))
        self.dropout = nn.Dropout(dropout)

    def forward(self, token_ids):  # (B,L)
        B, L = token_ids.shape
        x = self.emb(token_ids)  # (B,L,d)
        x = x + self.pe[:L, :].unsqueeze(0)  # broadcast
        return self.dropout(x)

# 简单验算
B, L, V, d = 2, 5, 100, 16
m = TokenEmbeddingWithPos(V, d, max_len=64)
tok = torch.randint(0, V, (B, L))
out = m(tok)
print(out.shape)  # (2,5,16)
```
