# 题目：实现 Scaled Dot-Product Attention(单头版)

> 目标 根据公式实现“缩放点积注意力”（单头）。要求同时支持：
>
> - **padding mask**：把 padding 的位置权重置为 0；
> - **causal mask**：自回归时，禁止看到未来位置信息。

## 数学定义

给定查询 $Q \in \mathbb{R}^{T_q \times d}$，键 $K \in \mathbb{R}^{T_k \times d}$，值 $V \in \mathbb{R}^{T_k \times d_v}$：

$$
\text{Attention}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d}} + M\right)V
$$

其中 $M$ 是 mask，在被屏蔽处为 $-\infty$，其余为 0。

## 额外的输入/输出规定

- 输入：`Q, K, V` 形状为 `(B, T, d)`（为简便设 `d_v = d`），可选：
  - `padding_mask`: `(B, T)`，True/1 表示“该位置是 padding，需要屏蔽”；
  - `causal`: bool。
- 输出：`context` 形状 `(B, T, d)` 以及注意力权重 `att_weights` 形状 `(B, T, T)`。

## 实现要求

- 需考虑数值稳定性(softmax 前减最大值)。
- 允许传入 `dropout_p`(对注意力权重做 dropout)。
- 不允许使用 `torch.nn.MultiheadAttention` 等现成实现。

## 参考答案

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ScaleAttention(nn.Module):
    def __init__(self, d: int, dropout: float):
        super().__init__()

        self.scale = d ** 0.5
        self.dropout = nn.Dropout(dropout)

    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,
                padding_mask: torch.Tensor = None, causal: bool = False):
        """
        Q, K, V: (B, T, D)
        padding_mask: (B, T)
        True: mask
        """
        B,T,D = Q.shape
        att_scores = torch.matmul(Q, K.transpose(-2,-1)) / self.scale # (B,T,T)

        if padding_mask is not None:
            key_mask = padding_mask.unsqueeze(1).expand(-1, T, -1) # (B,1,T) → (B,T,T)
            att_scores = att_scores.masked_fill(padding_mask, float("-inf"))  # (B,T,T)
        if causal:
            causal_mask = torch.triu(torch.ones(T,T,device=att_scores.device,dtype=bool), diagonal=1)
            att_scores = att_scores.masked_fill(causal_mask, float("-inf")) # 屏蔽未来信息，上三角

        att_scores = att_scores - att_scores.amax(dim=-1, keepdim=True) # 数值稳定, 不变分布只平移中心, 降低exp溢出风险

        att_weights = F.softmax(att_scores, dim=-1)
        att_weights = self.dropout(att_weights) # 对注意力权重做 dropout
        context = torch.matmul(att_weights, V)
        return context, att_weights
```
